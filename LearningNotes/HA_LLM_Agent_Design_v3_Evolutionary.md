# HA-LLM-Agent: Evolutionary Hybrid Automaton Discovery

## 1. Introduction

This document outlines the design for an autonomous **Hybrid Automaton (HA) Inference Agent** using Large Language Models (LLMs). The goal is to evolve the current `Dainarx` project from a semi-automated tool (requiring manual priors like `order` or `kernels`) into a fully autonomous "Scientific Discovery" agent.

The system addresses two specific challenges:
1.  **Parameter Autonomy:** Removing the need for manual JSON configuration of model hyperparameters.
2.  **Visual Ambiguity:** Handling systems where mode transitions are not visually apparent (e.g., Duffing) alongside those that are (e.g., Bouncing Ball).

## 2. Core Decision: Output Format

The agent must output a representation of the inferred physical system. We compared two approaches:

### Option A: JSON Output (Current Standard)
*   **Pros:** Fits existing `main.py` loader; language-agnostic.
*   **Cons:** LLMs struggle to strictly adhere to complex JSON schemas for mathematical expressions; rigid structure limits the expression of novel non-linear terms or complex guard logic; requires a complex parsing layer.

### Option B: Python Class Synthesis (Selected)
*   **Pros:**
    *   **Native to LLMs:** Models like Claude 3.5 Sonnet or GPT-4o excel at writing executable code.
    *   **Expressiveness:** Can define arbitrary non-linear dynamics (e.g., `np.sin(x) * x**2`) without a pre-defined kernel list.
    *   **Execution:** The "Model" is directly executable, simplifying the verification step.
*   **Cons:** Security risks (requires sandboxing); requires a "Converter" to map back to JSON if legacy tools need it.

**Decision:** The Agent will function as a **Program Synthesizer**, generating a **Python Class** that models the system. This aligns with the "AI Scientist" paradigm where the model writes the simulator to verify its hypothesis.

## 3. Architecture

We will utilize **Hugging Face `smolagents`** for the orchestration. This framework is code-centric, allowing the agent to write its thoughts and tool calls in Python.

### 3.1. The `InferredHA` Interface
The agent will generate Python code adhering to a strict interface to ensure it can be simulated and evaluated.

```python
# Interface Definition (The Agent must implement this)
class BaseHybridAutomaton:
    def __init__(self):
        self.current_mode = 0
        self.state = None

    def predict_next(self, current_state: np.ndarray, dt: float) -> np.ndarray:
        """Calculate next state based on current mode dynamics."""
        pass

    def check_transitions(self, current_state: np.ndarray) -> int:
        """Return new mode index (or current one if no transition)."""
        pass
```

### 3.2. Tools Available to the Agent

1.  **`VisualPerceptionTool`**:
    *   **Input:** Plot of the trajectory (Image).
    *   **Function:** Uses a Vision-Language Model (VLM) to estimate $N_{modes}$ and rough transition timestamps.
    *   **Output:** `{"estimated_modes": 2, "transition_hints": ["t=12.5 (sharp turn)"]}`

2.  **`DataStatsTool`**:
    *   **Input:** NPZ file path.
    *   **Function:** Returns basic statistics (dimensions, value ranges, variance) to help set scale factors.

3.  **`SimulationEvaluator`**:
    *   **Input:** The Python code string generated by the Agent.
    *   **Function:** 
        1. Instantiates the generated class.
        2. Runs it against the training portion of the NPZ data.
        3. Returns **MSE (Mean Squared Error)** and a **Residual Plot** (time vs. error).

4.  **`SymbolicRegressor`** (Optional):
    *   **Input:** A segment of data $(X, Y)$.
    *   **Function:** Uses `pysr` or a simplified regression call to suggest equations if the LLM fails to guess the math.

## 4. Inference Workflow: The "Hypothesis-Verify-Refine" Loop

The agent follows an evolutionary loop inspired by the *SR-Scientist* methodology.

### Phase 1: Initial Hypothesis (Perception)

The agent looks at the data to form a prior.

*   **Scenario A: Visible Transitions (e.g., Bouncing Ball)**
    1.  `VisualPerceptionTool` sees sharp corners or discontinuities.
    2.  Agent hypothesizes: "This looks like a Hybrid System with 2 modes."
    3.  Agent drafts `class BouncingBall(BaseHybridAutomaton)` with 2 modes and a guard condition based on $x \approx 0$.

*   **Scenario B: Invisible Transitions (e.g., Duffing)**
    1.  `VisualPerceptionTool` sees a smooth chaotic spiral.
    2.  Agent hypothesizes: "Visually smooth. I will start with a **Single Mode** assumption."
    3.  Agent drafts `class Duffing(BaseHybridAutomaton)` with 1 mode (e.g., simple linear dynamics).

### Phase 2: Verification & Refinement (The Scientist Loop)

1.  **Code Generation:** Agent writes the Python class.
2.  **Evaluation:** `SimulationEvaluator` runs the code.
3.  **Analysis:**
    *   If MSE is low (< threshold): **Success**.
    *   If MSE is high: Agent analyzes the **Residual Plot**.
    *   **Crucial Step for "Invisible" Transitions:** 
        *   If the Single Mode assumption (Duffing) yields high error *specifically in certain regions*, the Agent interprets this as "Hidden Mode Switching".
        *   Action: "The 1-mode hypothesis failed. I will split the data into 2 clusters based on error magnitude and fit separate dynamics."

### Phase 3: Finalization

Once the error is minimized, the Agent:
1.  Outputs the final Python Class.
2.  (Optional) Converts the Python logic (Equation + Guards) into the standard `automata/*.json` format for compatibility with the C++ or legacy Python simulator.

## 5. Implementation Roadmap

### Step 1: Environment Setup
*   Install `smolagents`.
*   Create the `BaseHybridAutomaton` abstract class.

### Step 2: Tool Implementation
*   Wrap `ChangePoints.py` logic into a tool the agent can *optionally* call if it decides mathematical splitting is needed.
*   Create the `SimulationEvaluator` which acts as the "Sandbox".

### Step 3: Prompt Engineering
*   Design the `system_prompt` to force the "Scientist" persona.
*   *Key Instruction:* "You are a physicist. Start simple. If the data defies your equation, assume a phase transition occurred."

### Step 4: Testing
*   **Test Case 1 (Visually Obvious):** Feed `data_plot/ATVA/ball`. Expect Agent to use Vision to find the bounce.
*   **Test Case 2 (Visually Hidden):** Feed `data_duffing/`. Expect Agent to start with 1 mode, fail, and evolve to 2 modes.

## 6. Compatibility Note

*   **JSON Compatibility:** The generated Python class is the "Gold Standard". To maintain backward compatibility, we will ask the LLM to run a final step: "Summarize your Python class into a JSON object matching this schema: `{var: [], mode: [], edge: []}`".
*   **Simulator:** We will gradually move `main.py` to use the generated Python classes for simulation, as they are faster and more flexible than parsing JSON strings.